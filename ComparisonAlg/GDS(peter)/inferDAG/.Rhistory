n = nrow(Sigma)
A = matrix(0, ncol = n, nrow = n)
W = matrix(0, ncol = n, nrow = n)
mbs = list()
for( i in  1:n) mbs[[i]] = get_markov_blanket(O_hat, i, inv_cov_threshold=inv_cov_threshold)
# position of a node in the dag order.
pos_in_dag_order = sort(dag_order, index.return=TRUE)$ix #
for( i in 2:n){
v = dag_order[i] # Variable for which we
# want to parents and parameters.
# print "learning parameters for node:", v
S = mbs[[v]][mbs[[v]] < pos_in_dag_order[v]]
if(length(S) > 0 ){
theta_v = ols_estimate(Sigma, S, v)
for( j in 1:length(theta_v) ) {
u = S[j]
if( abs(theta_v[j]) > inv_cov_threshold){
A[v, u] = 1
W[v, u] = theta_v[j]
}
}
}
}
return(list(A = A, W = W, vars = vars))
}
learn_gbn = function(X, Sigma, reg_param = 0.05,
inv_cov_threshold,
use_marginal_variance_for_dag_order=FALSE){
#   """
# Learn a GBN from data.
# :param Sigma: Sample covariance matrix.
# :param reg_param:
# :param inv_cov_threshold:
# :return: An @GaussianBayesianNetwork instance.
# """
# Sigma = quantize(Sigma, threshold=inv_cov_threshold)
tryCatch(
{
O_hat = inv_cov_estimate_clime(X, threshold = reg_param)
},
error = function(e){
# print( e.message)
message( "Failed to learn inverse covariance matrix.")
return( NULL)
}
)
tryCatch(
{
if(!use_marginal_variance_for_dag_order){
result= get_dag_order(Sigma, O_hat, inv_cov_threshold= inv_cov_threshold)
dag_order = result[[1]]
v = result[[2]]
# ordering estimation in population.
# result = get_dag_order(Sigma = solve(diag(p)-B) %*% diag(noiseVar) %*% solve(diag(p) -t(B)), O_hat = (diag(p) -t(B)) %*% diag(1/noiseVar) %*% (diag(p)-B), inv_cov_threshold= INV_COVARIANCE_THRESHOLD)
# 출력형식 확인.
}else{
# Use marginal variance to figure out DAG order.
dag_order = sort(diag(Sigma), index.return = T)$ix
v = rep(1,nrow(Sigma))
}
},
error = function(e){
message("Failed to learn DAG order.")
# print(e.message)
# tb.print_exc()
return( NULL)
}
)
# print "DAG order:", dag_order
bn = get_bn_from_order(Sigma, O_hat, dag_order, v,
inv_cov_threshold=inv_cov_threshold)
return(
list( DAG = bn$'A',
Ordering = dag_order
)
)
}
GBN_Algorithm = function(X, Sigma, reg_param= 0.05,
inv_cov_threshold,
use_marginal_variance_for_dag_order=FALSE, graph = NULL){
#### Start ###
Runtime = proc.time()[3]
result = learn_gbn(X, Sigma, reg_param,
inv_cov_threshold=inv_cov_threshold,
use_marginal_variance_for_dag_order=FALSE)
Runtime = proc.time()[3] - Runtime
Estimated_G = result$DAG
est_MEC = dag2cpdagAdj(Estimated_G)
Ordering = result$Ordering
if( !is.null(graph) ){
B = graph
B[B!=0] =1
MEC = B + t(B)
evaluation_result_GSEM = evaluation_fun( B, Estimated_G )
evaluation_result_GSEM_MEC = evaluation_fun( dag2cpdagAdj(B), est_MEC)
}
return(
list( DAG_Evaluation = evaluation_result_GSEM,
MEC_Evaluation = evaluation_result_GSEM_MEC,
Oracle_Evaluation = NULL,
DAG = Estimated_G,
Ordering = Ordering,
Time = Runtime)
)
}
standardize = function(X){
# standardize data.
n = nrow(X)
m = apply(X ,2, mean)
s = apply(X , 2, function(X){sqrt(var(X)*(n-1)/n)})
#python 은 분모를 n으로 사용.
Z = (X - m)/s
return( Z)
}
select_model = function(X, reg_params, thresholds){
# """
# Split data into train and test. Learn multiple GBNs
# for all combinations regularization parameter and thresholds.
# Return the model that has the highest test loglikelihood.
#
# :param X: data (m x n), where m is #samples, n is #nodes.
# :param reg_params:
# :param thresholds:
# :return: The best model.
# """
X = as.matrix(X)
Z = standardize(X)
# split data into train and test.
train_idx = sample( 1:nrow(Z ), round(0.5*nrow(Z)) )
Z_train = Z[train_idx,] ; Z_test = Z[-train_idx,]
print(paste("training data size:", paste(dim(Z_train),collapse =  'x')))
print(paste("test data size:", paste(dim(Z_test) , collapse  = 'x')))
# Compute sample covariance matrix.
C_train = (1/nrow(Z_train)) * t(Z_train)%*%(Z_train)
best_r = 0; best_t = 0 ;  min_neg_log_lik = Inf
for( r in reg_params){
for(t in thresholds){
l = Inf
bnh = learn_gbn(Z_train, C_train, r, inv_cov_threshold=t)
if(!is.null(bnh))l = -bnh.log_likelihood(Z_test) ########## 이부분만 수정
print( paste0("reg_param:",r,", threshold: ",t,", test negative log likelihood: ",round(l,3)))
if( l < min_neg_log_lik){
best_r = r ; best_t = t }
}
}
C = (1./nrow(Z)) * t(Z)%*%Z
bn = learn_gbn(Z, C, best_r * sqrt(1./2), t)
return(list(bn = bn, best_r = best_r, best_t = best_t)) #  형식고려
}
############## Evaluation Methods: Ref, Eunho Yang  ###############
evaluation_fun = function(true_graph, estimated_graph){
#Precision: the fraction of all predicted (directed) edges that are actually present in the true DAG
#recall: the fraction of directed edges in the true DAG that the method was able to recover.
#true positives: the numbers of correctly identified edges.
#true negatives: the numbers of correctly identified absence of edges.
#false positives: the numbers of edges were falsely added.
#false negatives: the numbers of edges were falsely added falsely missing.
true_graph_edge = true_graph+t(true_graph)
estimated_graph_edge = estimated_graph+t(estimated_graph)
true_graph_total_edges = sum(true_graph)
estimated_graph_total_edges = sum(estimated_graph)
precisition =  sum( (true_graph + estimated_graph) == 2 )/sum(estimated_graph)
recall = sum( (true_graph + estimated_graph) == 2 )/sum(true_graph)
precisition_edge =  sum( (true_graph_edge + estimated_graph_edge) == 2 )/sum(estimated_graph_edge)
recall_edge = sum( (true_graph_edge + estimated_graph_edge) == 2 )/sum(true_graph_edge)
true_positives = sum( (true_graph + estimated_graph) == 2 )
true_negatives = sum( (true_graph + estimated_graph) == 0 )
false_positives = sum( true_graph < estimated_graph)
false_negatives = sum( true_graph > estimated_graph)
true_positives_edge = sum( (true_graph_edge + estimated_graph_edge) == 2 )/2
true_negatives_edge = sum( (true_graph_edge + estimated_graph_edge) == 0 )/2
false_positives_edge = sum( true_graph_edge < estimated_graph_edge)/2
false_negatives_edge = sum( true_graph_edge > estimated_graph_edge)/2
hamming_dist = sum(true_graph!=estimated_graph)
hamming_dist_edge = sum( true_graph_edge != estimated_graph_edge ) /2
hamming_dist_ordering = hamming_dist - hamming_dist_edge
return( c(precisition = precisition,
recall = recall,
precisition_edge = precisition_edge,
recall_edge = recall_edge,
true_positives = true_positives,
true_negatives = true_negatives,
false_positives = false_positives,
false_negatives = false_negatives,
true_positives_edge = true_positives_edge,
true_negatives_edge = true_negatives_edge,
false_positives_edge = false_positives_edge,
false_negatives_edge = false_negatives_edge,
hamming_dist = hamming_dist,
hamming_dist_edge = hamming_dist_edge,
hamming_dist_ordering  = hamming_dist_ordering,
true_graph_total_edges = true_graph_total_edges,
estimated_graph_total_edges = estimated_graph_total_edges) )
}
############## Estimated directed graph: estimated_graph_fun ####################
estimated_graph_fun = function(directed_graph_edges, ordering){
for(i in 1:length(ordering)) directed_graph_edges[ordering[1:i],ordering[i]]<-0
return( directed_graph_edges )
}
############## DAG2CPDAGAdj ##############
dag2cpdagAdj <- function(Adj){
library(graph)
#library(pcalg)
library(bnlearn)
d <- as(Adj, "graphNEL")
cpd <- cpdag(as.bn(d) )
result<- amat(cpd)
# if pcalg is allowed, the following code works.
#cpd <- dag2cpdag(d)
#result <- as(cpd, "matrix")
return(result)
}
res = matrix(0, nrow = 17, ncol =3)
for(i in 1:10){
p = 10; n = 1000;
beta_min = 0.20; beta_max = 1.00
graph_type = 1
noiseVar <- rep(1, p)
noiseVar <- runif(p, 1, 1)
raw = GSEM_generator( n, p, d =1, noiseVar, dist = "Gaussian", beta_min, beta_max, graph_type, structure = NULL, seed = sample(100,1))
data = raw$x
B = raw$true_Matrix
out = GSEM_Algorithm(data, alpha = 0.0001, direction ="forward", graph = B)
out2 = GSEM_Algorithm(data, alpha = 0.0001, direction ="backward", graph = B)
out3 = GBN_Algorithm(X = data, Sigma = cov(data) *(n-1)/n,
reg_param = 0.01,
inv_cov_threshold= 0.1,
use_marginal_variance_for_dag_order=FALSE, graph = B)
#get_dag_order(Sigma = solve(diag(p)-B) %*% diag(noiseVar) %*% solve(diag(p) -t(B)), O_hat = (diag(p) -t(B)) %*% diag(1/noiseVar) %*% (diag(p)-B), inv_cov_threshold= INV_COVARIANCE_THRESHOLD)
res = res + cbind(out[[1]], out2[[1]], out3[[1]])
rbind(out[[5]], out2[[5]], out3[[5]])
}
res
#######
####
### GSEM generator function ###
GSEM_generator = function( n, p, d =2, noiseVar, dist = "Gaussian", beta_min, beta_max, graph_type, structure = NULL, seed = 1){
set.seed(seed)
## Generating data with p, n ##
B = matrix( 0, p ,p )
## various type of graphs ##
#tree
if( graph_type == 1 ) {
for(i in 2:p){
parents_i = (i-1)
B[i,parents_i ] = runif( length(parents_i) , beta_min,  beta_max)* sample( c(-1,1), length(parents_i), replace = T)
}
}
#expected number of parents
if( graph_type == 3 ) {
prob = d / (p-1)
for(i in 2:p){
parents_i = 1:(i-1)
B[i,parents_i ] = runif( length(parents_i) , beta_min,  beta_max)* sample( c(-1,1), length(parents_i), replace = T)
B[i,parents_i ] = B[i,parents_i ] * sample( c(0,1), length(parents_i), replace = T, prob = c(1-prob, prob))
}
}
#ratio number of parents
if( graph_type == 4 ) {
for(i in 2:p){
parents_i = 1:(i-1)
B[i,parents_i ] = runif( length(parents_i) , 0,  beta_max)* sample( c(-1,1), length(parents_i), replace = T)
}
B[ abs(B)< abs(beta_min) ] = 0
}
#fixed number of parents
if( graph_type == 5 ) {
for(i in 2:p){
parents_i = sample( 1:(i-1), min(i-1, d),replace = FALSE )
B[i,parents_i ] = runif( length(parents_i) , beta_min,  beta_max)* sample( c(-1,1), length(parents_i), replace = T)
}
}
### Sample with respect to the given distributions ###
## link function: exponential; it can be any valid function
x= matrix( 0, n, p)
i = 1
x[,i] = rnorm(n, 0, noiseVar[i])
for( i in 2:p){
eta_i = colSums( B[i,1:(i-1) ]*t( x[,1:(i-1)] ) )
beta_0 = 0
theta_i = eta_i + beta_0
x[,i] = rnorm(n, theta_i, noiseVar[i])
}
x = as.data.frame(x)
DAGsamples = list(x= x, true_Matrix= B )
### path ###
#setwd("C:/2019GaussianSEM_SimulationResult/Data/Homo")
#setwd("C:/2019GaussianSEM_SimulationResult/Data/Hetero")
#GaussianDAG_filename = paste0("GaussianSEM_p",p,"_seed",seed,".Rdata")
#save(DAGsamples, file = GaussianDAG_filename)
return( DAGsamples)
}
#####GSEM Learning Algorithm via Conditional Independence Test#########
GSEM_Algorithm = function(data, alpha = 0.05, direction ="forward", graph = NULL){
library(bnlearn)
###################
X = as.matrix( data )
p = ncol(X)
n = nrow(X)
RemNode = 1:p
pi_GSEM = NULL
Estimated_G = Estimated_O = matrix(0, p ,p)
####
Runtime = proc.time()[3]
#### Step 1): Finding the Ordering ####
if(direction == "forward"){
Ordering = Forward_Learning_fun(X)
}else if(direction =="backward"){
Ordering = Backward_Learning_fun(X)
}
#### Step 2): Finding the Parents ####
for(m in 2:p){
j = Ordering[m]
for(k in Ordering[1:(m-1)]){
if(m > 3){
S = setdiff( Ordering[ 1:(m-1)], k )
parent_pvalue = ci.test(X[,j], X[,k], X[,S], test = "cor")$p.value
}else{
parent_pvalue = ci.test(X[,j], X[,k], test = "cor")$p.value
}
if(parent_pvalue < alpha){
Estimated_G[j, k] = 1
}
}
}
####
Runtime = proc.time()[3] - Runtime
print(paste("It takes: ", Runtime))
####
est_MEC = dag2cpdagAdj(Estimated_G)
####
if( !is.null(graph) ){
B = graph
B[B!=0] =1
MEC = B + t(B)
evaluation_result_GSEM = evaluation_fun( B, Estimated_G )
evaluation_result_GSEM_MEC = evaluation_fun( dag2cpdagAdj(B), est_MEC)
}
return(
list( DAG_Evaluation = evaluation_result_GSEM,
MEC_Evaluation = evaluation_result_GSEM_MEC,
Oracle_Evaluation = NULL,
DAG = Estimated_G,
Ordering = Ordering,
Time = Runtime)
)
}
#################################################
Forward_Learning_fun = function(X){
#### Step 1): Finding the Ordering ####
Ordering = rep(0, p)
RemNodes = 1:p
k = 1
Ordering[k] = which.min( sapply(RemNodes, function(j) var(X[,j]) ) )
RemNodes = setdiff(RemNodes, Ordering)
while( length(RemNodes) > 1 ){
k = k + 1
scores = sapply(RemNodes, function(j) sum(lm(X[,j]~X[,Ordering[1:(k-1)]])$resid^2)/n )
#print(scores)
Ordering_j = which( scores == min(scores) )
if( length(Ordering_j) > 1){
Ordering[k] = RemNodes[sample(Ordering_j, 1)]
}else{
Ordering[k] = RemNodes[Ordering_j]
}
RemNodes = setdiff(RemNodes, Ordering)
}
Ordering[p] = RemNodes
return( Ordering )
}
Backward_Learning_fun = function(X){
#### Step 1): Finding the Ordering ####
Ordering = rep(0, p)
RemNodes = 1:p
k = p
while( length(RemNodes) > 1 ){
scores = sapply(RemNodes, function(j) sum(lm(X[,j]~X[,setdiff(RemNodes,j)])$resid^2)/n )
#print(scores)
Ordering_j = which( scores == max(scores) )
if( length(Ordering_j) > 1){
Ordering[k] = RemNodes[sample(Ordering_j, 1)]
}else{
Ordering[k] = RemNodes[Ordering_j]
}
k = k - 1
RemNodes = setdiff(RemNodes, Ordering)
}
Ordering[1] = RemNodes
return( Ordering )
}
##################################
# Evaluation_Algorithm(20181005) is required #
##### GSEM_simulation_fun: Simulations for recovering PSEM ####
GSEM_simulation_fun = function(seed, n_real, p_real, alpha = 0.05 ){
#This algorithm is for simulations using Gaussian SEM Learning_algorithm.
# Gaussian, beta_jk = (-2,-0.25) \cup (0.25, 2) for Homo , and  (-2,-1) \cup (1, 2) for Hetero
#setwd("C:/2019GaussianSEM_SimulationResult/Data/Homo")
#setwd("C:/2019GaussianSEM_SimulationResult/Data/Hetero")
#p = 500;
setwd("C:/2019GaussianSEM_SimulationResult/Data/NonFaithful");p = 3
GaussianDAG_filename = paste0("GaussianSEM_p",p,"_seed",seed,".Rdata")
load(GaussianDAG_filename)
synthetic.graph =DAGsamples
graph = synthetic.graph$true_Matrix[1:p_real, 1:p_real]
data = synthetic.graph$x[1:n_real,1:p_real]
result_GSEM = GSEM_Algorithm(data = data, alpha = alpha, graph = graph)
return(result_GSEM)
}
#####GSEM Learning Algorithm via L1 regularized Regression #######################
GSEM_Algorithm2 = function(data, graph = NULL, lambda.lasso = 0.4, sparsity_level = 0.18){
library(glmnet)
tuning.parameter <- lambda.lasso
###################
X = as.matrix( data )
RemNode = 1:p
pi_GSEM = NULL
ScoreMatrix = matrix(0, p, p)
#### Finding Ordering ####
Time_GSEM = proc.time()[3]
#### ordering estimation
Ordering = rep(0, p)
Estimated_G = Estimated_O = matrix(0, p ,p)
RemNodes = 1:p
k = 1
Ordering[k] = which.min( sapply(RemNodes, function(j) var(X[,j]) ) )
RemNodes = setdiff(RemNodes, Ordering)
while( length(RemNodes) >1 ){
k = k + 1
Ordering[k] = RemNodes[which.min( sapply(RemNodes, function(j) sum(lm(X[,j]~X[,Ordering[1:(k-1)]])$resid^2)/n ) )]
RemNodes = setdiff(RemNodes, Ordering)
}
Ordering[p] = RemNodes
#### paremnts estimation
glm0 = glmnet(X[,rep(Ordering[1],2)], X[,Ordering[2]], family = "gaussian", alpha = 1, lambda = tuning.parameter )$beta
if( sum( glm0[,1] != 0)   ){
Estimated_G[Ordering[2], Ordering[1]] = 1
}
for(k in 3:p){
glm0 = glmnet(X[, Ordering[1:(k-1)]], X[,Ordering[k]], family = "gaussian", alpha = 1, lambda = tuning.parameter )$beta
if( sum( glm0[,1] != 0)   ){
parents = which( glm0[,1] != 0 )
Estimated_G[Ordering[k], Ordering[parents]] = 1
}
}
if( !is.null(graph) ){
B = graph
B[B!=0] =1
MEC = B + t(B)
Oracle_DAG = estimated_graph_fun( MEC, Ordering )
evaluation_result_GSEM = evaluation_fun( B, Estimated_G )
evaluation_result_GSEM_MEC = evaluation_fun( dag2cpdagAdj(B), dag2cpdagAdj(Estimated_G) )
evaluation_result_GSEM_Oracle = evaluation_fun( B, Oracle_DAG )
}
return( list( DAG_Evaluation = evaluation_result_GSEM,
MEC_Evaluation = evaluation_result_GSEM_MEC,
Oracle_Evaluation = evaluation_result_GSEM_Oracle,
DAG = Estimated_G,
Ordering = Ordering ,
Time = Time_GSEM) )
}
####################
#RawData = GSEM_generator( n, p, d, noiseVar, beta_min, beta_max, graph_type, seed = 1)
#GSEM_Algorithm(data, graph, lambda.lasso = 0.4, sparsity_level = 1)$DAG_Evaluation
###
res = matrix(0, nrow = 17, ncol =3)
for(i in 1:10){
p = 10; n = 1000;
beta_min = 0.20; beta_max = 1.00
graph_type = 1
noiseVar <- rep(1, p)
noiseVar <- runif(p, 1, 1)
raw = GSEM_generator( n, p, d =1, noiseVar, dist = "Gaussian", beta_min, beta_max, graph_type, structure = NULL, seed = sample(100,1))
data = raw$x
B = raw$true_Matrix
out = GSEM_Algorithm(data, alpha = 0.0001, direction ="forward", graph = B)
out2 = GSEM_Algorithm(data, alpha = 0.0001, direction ="backward", graph = B)
out3 = GBN_Algorithm(X = data, Sigma = cov(data) *(n-1)/n,
reg_param = 0.01,
inv_cov_threshold= 0.1,
use_marginal_variance_for_dag_order=FALSE, graph = B)
#get_dag_order(Sigma = solve(diag(p)-B) %*% diag(noiseVar) %*% solve(diag(p) -t(B)), O_hat = (diag(p) -t(B)) %*% diag(1/noiseVar) %*% (diag(p)-B), inv_cov_threshold= INV_COVARIANCE_THRESHOLD)
res = res + cbind(out[[1]], out2[[1]], out3[[1]])
rbind(out[[5]], out2[[5]], out3[[5]])
}
res
out3 = GBN_Algorithm(X = data, Sigma = cov(data) *(n-1)/n,
reg_param = 0.01,
inv_cov_threshold= 0.1,
use_marginal_variance_for_dag_order=FALSE, graph = B)
out3
######
res = matrix(0, nrow = 17, ncol =3)
for(i in 1:10){
p = 10; n = 1000;
beta_min = 0.20; beta_max = 1.00
graph_type = 1
noiseVar <- rep(1, p)
noiseVar <- runif(p, 1, 1)
raw = GSEM_generator( n, p, d =1, noiseVar, dist = "Gaussian", beta_min, beta_max, graph_type, structure = NULL, seed = sample(100,1))
data = raw$x
B = raw$true_Matrix
out = GSEM_Algorithm(data, alpha = 0.0001, direction ="forward", graph = B)
out2 = GSEM_Algorithm(data, alpha = 0.0001, direction ="backward", graph = B)
out3 = GBN_Algorithm(X = data, Sigma = cov(data) *(n-1)/n,
reg_param = 0.01,
inv_cov_threshold= 0.1,
use_marginal_variance_for_dag_order=FALSE, graph = B)
#get_dag_order(Sigma = solve(diag(p)-B) %*% diag(noiseVar) %*% solve(diag(p) -t(B)), O_hat = (diag(p) -t(B)) %*% diag(1/noiseVar) %*% (diag(p)-B), inv_cov_threshold= INV_COVARIANCE_THRESHOLD)
res = res + cbind(out[[1]], out2[[1]], out3[[1]])
rbind(out[[5]], out2[[5]], out3[[5]])
}
res
